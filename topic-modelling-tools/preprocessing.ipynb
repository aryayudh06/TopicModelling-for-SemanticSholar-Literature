{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv('../data/universitas_brawijaya_scholar_results2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove \"[PDF]\" or \"[pdf]\" using regular expression (case-insensitive)\n",
    "    text = re.sub(r'\\[pdf\\]', '', text, flags=re.IGNORECASE)\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords.words('english')]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing function to the 'Title' column\n",
    "df['Processed_Title'] = df['Title'].apply(preprocess_text)\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "\n",
    "# Fit and transform the processed text data\n",
    "dtm = vectorizer.fit_transform(df['Processed_Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for Topic #0:\n",
      "['yield', 'effect', 'stock', 'study', 'soil', 'tourism', 'east', 'development', 'java', 'indonesia']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #1:\n",
      "['use', 'quality', 'growth', 'consumer', 'tourism', 'brand', 'perceived', 'intention', 'study', 'effect']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #2:\n",
      "['strategy', 'used', 'learning', 'language', 'program', 'universitas', 'english', 'brawijaya', 'study', 'student']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #3:\n",
      "['indonesia', 'community', 'area', 'change', 'main', 'study', 'character', 'land', 'analysis', 'movie']\n",
      "\n",
      "\n",
      "Top 10 words for Topic #4:\n",
      "['plant', 'malang', 'land', 'organic', 'potential', 'mining', 'case', 'study', 'using', 'soil']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the LDA model\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "\n",
    "# Fit the LDA model on the document-term matrix\n",
    "lda.fit(dtm)\n",
    "\n",
    "# Display the top words for each topic\n",
    "for index, topic in enumerate(lda.components_):\n",
    "    print(f'Top 10 words for Topic #{index}:')\n",
    "    print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  Topic\n",
      "0  [PDF] Management of Public Information Disclos...      2\n",
      "1  [PDF] Director Rector of Universitas Brawijaya...      2\n",
      "2  Refusal Strategies Used By Male And Female Stu...      2\n",
      "3  Achieving World Class University Through Inter...      2\n",
      "4  the Translation of Demonstrative References fr...      2\n"
     ]
    }
   ],
   "source": [
    "# Assign the most relevant topic to each document\n",
    "topic_results = lda.transform(dtm)\n",
    "df['Topic'] = topic_results.argmax(axis=1)\n",
    "\n",
    "# Display the first few rows of the dataframe with assigned topics\n",
    "print(df[['Title', 'Topic']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
